"""
é¡¹ç›®é…ç½®æ–‡ä»¶
ç”¨äºç»Ÿä¸€ç®¡ç†æ¨¡å‹è·¯å¾„ã€ç¼“å­˜è·¯å¾„ç­‰é…ç½®
"""

import os

# ==========================================
# æ¨¡å‹ç¼“å­˜è·¯å¾„é…ç½®
# ==========================================

# æ–¹æ¡ˆ1ï¼šå¦‚æœä½ çš„æ¨¡å‹å·²ç»ä¸‹è½½åˆ° D ç›˜ï¼Œè®¾ç½®è¿™ä¸ªè·¯å¾„
# ä¾‹å¦‚ï¼šD:\AI_Models\huggingface æˆ– D:\models\transformers
# æ³¨æ„ï¼šè™½ç„¶å«.cacheï¼Œä½†å®ƒæ˜¯æ°¸ä¹…å­˜å‚¨ï¼Œä¸ä¼šè¢«ç³»ç»Ÿæ¸…ç†
# å¦‚æœæƒ³æ”¹æˆæ›´ç›´è§‚çš„åå­—ï¼Œå¯ä»¥æ”¹ä¸º "D:/AI_LLM_Project/models"
CUSTOM_MODEL_CACHE = "D:/AI_LLM_Project/models"  # Dç›˜æ¨¡å‹æ°¸ä¹…å­˜å‚¨ç›®å½•

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨é»˜è®¤çš„ Hugging Face ç¼“å­˜è·¯å¾„
# Windows é»˜è®¤: C:\Users\{username}\.cache\huggingface
USE_CUSTOM_CACHE = True  # å·²å¯ç”¨ D ç›˜è‡ªå®šä¹‰è·¯å¾„

# ==========================================
# ç¯å¢ƒå˜é‡è®¾ç½®
# ==========================================

def setup_environment():
    """
    é…ç½®è¿è¡Œç¯å¢ƒ
    åœ¨æ‰€æœ‰è„šæœ¬å¼€å§‹æ—¶è°ƒç”¨è¿™ä¸ªå‡½æ•°
    """
    # è®¾ç½®é•œåƒæºï¼ˆå›½å†…åŠ é€Ÿ - é˜¿é‡Œäº‘é•œåƒï¼‰
    os.environ["HF_ENDPOINT"] = "https://mirrors.aliyun.com/huggingface"
    
    # å¦‚æœå¯ç”¨è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„
    if USE_CUSTOM_CACHE and CUSTOM_MODEL_CACHE:
        # Hugging Face ç¼“å­˜ï¼ˆHF_HOME æ˜¯ä¸»è¦é…ç½®ï¼ŒTRANSFORMERS_CACHE å·²åºŸå¼ƒï¼‰
        os.environ["HF_HOME"] = CUSTOM_MODEL_CACHE
        os.environ["HF_HUB_CACHE"] = os.path.join(CUSTOM_MODEL_CACHE, "hub")
        
        # Sentence-Transformers ç¼“å­˜
        os.environ["SENTENCE_TRANSFORMERS_HOME"] = os.path.join(CUSTOM_MODEL_CACHE, "sentence-transformers")
        
        print(f"âœ… å·²è®¾ç½®æ¨¡å‹ç¼“å­˜è·¯å¾„: {CUSTOM_MODEL_CACHE}")
    else:
        print("âœ… ä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„")
    
    return os.environ.get("HF_HOME", "default")

# ==========================================
# æ¨¡å‹åç§°é…ç½®
# ==========================================

# åŸºåº§æ¨¡å‹ - ä½¿ç”¨ ModelScope å·²ä¸‹è½½çš„æœ¬åœ°è·¯å¾„
# æ³¨æ„ï¼šè·¯å¾„ä¸­çš„ç‰¹æ®Šå­—ç¬¦æ˜¯ ModelScope çš„å‘½åè§„åˆ™
BASE_MODEL = "D:/AI_LLM_Project/models/qwen/Qwen2___5-0___5B-Instruct"

# å¦‚æœéœ€è¦é‡æ–°ä¸‹è½½æˆ–ä½¿ç”¨ HuggingFaceï¼Œå¯ä»¥æ”¹å›ï¼š
# BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"

# Embedding æ¨¡å‹ï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰- ä½¿ç”¨ ModelScope ä¸‹è½½çš„æœ¬åœ°è·¯å¾„
EMBEDDING_MODEL = "D:/AI_LLM_Project/models/modelscope/BAAI/bge-small-zh-v1___5"

# å¦‚æœ Embedding æ¨¡å‹ä¸å­˜åœ¨ï¼Œå¯ä»¥ä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬ï¼š
# EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"

# å¦‚æœä½ å·²ç»ä¸‹è½½äº†æ¨¡å‹åˆ°å…¶ä»–æœ¬åœ°ä½ç½®ï¼Œå¯ä»¥ç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾„
# ä¾‹å¦‚ï¼š
# BASE_MODEL = "D:/AI_Models/Qwen2.5-0.5B-Instruct"
# EMBEDDING_MODEL = "D:/AI_Models/bge-small-zh-v1.5"

# ==========================================
# æ•°æ®åº“è·¯å¾„é…ç½®
# ==========================================

DB_FILE = "company_data.db"
CHROMA_DB_PATH = "./chroma_db"

# ==========================================
# è®­ç»ƒè¾“å‡ºè·¯å¾„é…ç½®
# ==========================================

# å¾®è°ƒæ¨¡å‹ä¿å­˜è·¯å¾„
FINE_TUNED_MODEL_DIR = "./fine_tuned_model"
FINE_TUNED_MODEL_4BIT_DIR = "./fine_tuned_model_4bit"

# è®­ç»ƒæ•°æ®è·¯å¾„
TRAIN_DATA_SMALL = "train_data.jsonl"
TRAIN_DATA_LARGE = "train_data_large.jsonl"

# ==========================================
# ä½¿ç”¨ç¤ºä¾‹
# ==========================================

if __name__ == "__main__":
    print("="*60)
    print("é¡¹ç›®é…ç½®ä¿¡æ¯")
    print("="*60)
    
    cache_path = setup_environment()
    
    print(f"\nå½“å‰é…ç½®ï¼š")
    print(f"  ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜: {USE_CUSTOM_CACHE}")
    print(f"  è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„: {CUSTOM_MODEL_CACHE}")
    print(f"  å®é™…ç¼“å­˜è·¯å¾„: {cache_path}")
    print(f"  åŸºåº§æ¨¡å‹: {BASE_MODEL}")
    print(f"  Embeddingæ¨¡å‹: {EMBEDDING_MODEL}")
    print(f"  æ•°æ®åº“æ–‡ä»¶: {DB_FILE}")
    print(f"  å‘é‡æ•°æ®åº“: {CHROMA_DB_PATH}")
    
    print("\n" + "="*60)
    print("ğŸ’¡ å¦‚ä½•ä½¿ç”¨ï¼š")
    print("="*60)
    print("\n1. å¦‚æœæ¨¡å‹å·²ä¸‹è½½åˆ° D ç›˜ï¼š")
    print("   - ä¿®æ”¹ CUSTOM_MODEL_CACHE ä¸ºå®é™…è·¯å¾„")
    print("   - è®¾ç½® USE_CUSTOM_CACHE = True")
    print("   - æˆ–è€…ç›´æ¥ä¿®æ”¹ BASE_MODEL ä¸ºæœ¬åœ°è·¯å¾„")
    
    print("\n2. å¦‚æœéœ€è¦é‡æ–°ä¸‹è½½ï¼š")
    print("   - ä¿æŒ USE_CUSTOM_CACHE = False")
    print("   - è¿è¡Œè„šæœ¬ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°é»˜è®¤ä½ç½®")
    
    print("\n3. æŸ¥çœ‹å½“å‰æ¨¡å‹ä½ç½®ï¼š")
    import glob
    possible_paths = [
        os.path.join(os.path.expanduser("~"), ".cache", "huggingface", "hub"),
        "D:/AI_Models",
        CUSTOM_MODEL_CACHE
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            models = glob.glob(os.path.join(path, "**/config.json"), recursive=True)
            if models:
                print(f"\n   âœ… å‘ç°æ¨¡å‹: {path}")
                print(f"      æ¨¡å‹æ•°é‡: {len(models)}")
