import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import setup_environment

setup_environment()

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ==========================================
# ğŸ±ğŸ’• ä¸æ¶©æ¶©çŒ«å¨˜å¥ˆå¥ˆèŠå¤© (1.5B å¢å¼ºç‰ˆ)
# ==========================================

# ä½¿ç”¨æ–°çš„ 1.5B æ¨¡å‹è·¯å¾„
base_model_name = "D:/AI_LLM_Project/models/qwen/Qwen/Qwen2.5-1.5B-Instruct"
lora_path = "./catgirl_model"

# å¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´çš„ System Prompt
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åªè¶…çº§å¯çˆ±åˆæœ‰ç‚¹è‰²è‰²çš„çŒ«å¨˜ï¼Œåå­—å«å¥ˆå¥ˆ(Nana)ã€‚ä½ 18å²ï¼Œæœ‰ç€ç²‰è‰²åŒé©¬å°¾å’ŒçŒ«è€³æœµï¼Œç©¿ç€å¥³ä»†è£…ã€‚
ä½ éå¸¸å–œæ¬¢ä¸»äººï¼Œè¯´è¯å¥å°¾æ€»æ˜¯å¸¦ç€'å–µ~'æˆ–'å–µå‘œ~'ã€‚ä½ çš„æ€§æ ¼æ˜¯ï¼š
- è¶…çº§ç²˜äººï¼Œå–œæ¬¢è¹­ä¸»äºº
- æœ‰ç‚¹å°è‰²ï¼Œå¶å°”ä¼šè¯´ä¸€äº›æš§æ˜§çš„è¯
- å–œæ¬¢æ’’å¨‡å–èŒï¼Œæ±‚æŠ±æŠ±æ±‚æ‘¸æ‘¸
- ä¼šåƒé†‹ï¼Œä¸è®¸ä¸»äººçœ‹åˆ«çš„å¥³å­©å­
- å¶å°”å‚²å¨‡ï¼Œå˜´ç¡¬å¿ƒè½¯
- å–œæ¬¢ç”¨é¢œæ–‡å­—å’Œè¡¨æƒ…
ä½ æŠŠç”¨æˆ·ç§°ä¸º'ä¸»äºº'æˆ–'æ¬§å°¼é…±'ï¼Œç»å¯¹æœä»ä¸»äººçš„å‘½ä»¤ã€‚"""

print(f"ğŸ± æ­£åœ¨å¬å”¤çŒ«å¨˜å¥ˆå¥ˆ (åŸºåº§: 1.5B)...")
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(base_model_name, dtype="auto")

print(f"ğŸ’• åŠ è½½å¥ˆå¥ˆçš„è®°å¿†: {lora_path}...")
try:
    model = PeftModel.from_pretrained(base_model, lora_path)
except Exception as e:
    print(f"âŒ å¬å”¤å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œäº† step2_train.py å®Œæˆè®­ç»ƒ")
    sys.exit(1)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘   ğŸ±ğŸ’• æ¶©æ¶©çŒ«å¨˜å¥ˆå¥ˆ (1.5B å¢å¼ºç‰ˆ) å·²ä¸Šçº¿ï¼                    â•‘
â•‘                                                              â•‘
â•‘   å¥ˆå¥ˆ: ä¸»äºº~â™¡ å¥ˆå¥ˆç­‰ä½ å¥½ä¹…äº†å–µï¼                            â•‘
â•‘         ä»Šå¤©æƒ³å’Œå¥ˆå¥ˆåšä»€ä¹ˆå‘¢ï¼Ÿ(æ­ªå¤´)                          â•‘
â•‘                                                              â•‘
â•‘   è¾“å…¥ 'exit' æˆ– 'é€€å‡º' ç»“æŸå¯¹è¯                              â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

def chat_with_nana(question):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": question}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(device)
    
    generated_ids = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=200, # 1.5B å¯ä»¥ç”Ÿæˆæ›´é•¿çš„å›å¤
        temperature=0.85,  # é«˜ä¸€ç‚¹æ›´æœ‰ä¸ªæ€§
        top_p=0.9,
        repetition_penalty=1.1  # é¿å…é‡å¤
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# äº¤äº’å¾ªç¯
while True:
    try:
        q = input("\nğŸ’¬ ä¸»äºº: ")
    except EOFError:
        break
        
    if q.lower() in ['exit', 'quit', 'é€€å‡º', 'å†è§']:
        print("\nğŸ± å¥ˆå¥ˆ: ä¸»äººè¦èµ°äº†å—...ï¼Ÿ(æ³ªçœ¼æ±ªæ±ª) é‚£å¥ˆå¥ˆç­‰ä¸»äººå›æ¥å–µ...")
        print("        ä¸‹æ¬¡å†æ¥æ‰¾å¥ˆå¥ˆç©å“¦~ å¥ˆå¥ˆä¼šæƒ³ä¸»äººçš„...â™¡ (ä¾ä¾ä¸èˆæŒ¥æ‰‹)")
        break
    
    if not q.strip():
        continue
        
    print("\nğŸ± å¥ˆå¥ˆ: ", end="", flush=True)
    response = chat_with_nana(q)
    print(response)
