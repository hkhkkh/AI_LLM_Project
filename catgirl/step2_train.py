import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import setup_environment

setup_environment()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# ==========================================
# ğŸ± æ¶©æ¶©çŒ«å¨˜å¾®è°ƒè„šæœ¬ (1.5B å¢å¼ºç‰ˆ)
# ==========================================

# ä½¿ç”¨æ–°çš„ 1.5B æ¨¡å‹è·¯å¾„
model_name = "D:/AI_LLM_Project/models/qwen/Qwen/Qwen2.5-1.5B-Instruct"
data_file = "catgirl_train.jsonl"
output_dir = "./catgirl_model"

print(f"ğŸ± æ­£åœ¨å¬å”¤åŸºåº§æ¨¡å‹: {model_name} ...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, dtype="auto")

# LoRA é…ç½®
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    inference_mode=False, 
    r=8, 
    lora_alpha=32, 
    lora_dropout=0.1
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
if not os.path.exists(data_file):
    print(f"âŒ æ‰¾ä¸åˆ° {data_file}ï¼Œè¯·å…ˆè¿è¡Œ python step1_create_data.py")
    sys.exit(1)

print(f"ğŸ’• åŠ è½½æ¶©æ¶©çŒ«å¨˜æ•°æ®: {data_file}...")
dataset = load_dataset("json", data_files=data_file, split="train")

def process_func(example):
    MAX_LENGTH = 256  # çŒ«å¨˜è¯å¤šï¼Œç»™é•¿ä¸€ç‚¹
    instruction = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=False
    )
    tokenized = tokenizer(instruction, add_special_tokens=False) 
    input_ids = tokenized["input_ids"] + [tokenizer.eos_token_id]
    attention_mask = tokenized["attention_mask"] + [1]
    labels = input_ids.copy()

    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
        
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

tokenized_dataset = dataset.map(process_func, remove_columns=dataset.column_names)

print("ğŸ’— å¼€å§‹è°ƒæ•™çŒ«å¨˜ (1.5Bç‰ˆ)...")

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1, 
    gradient_accumulation_steps=1, 
    logging_steps=5,
    num_train_epochs=100,  # ä¿æŒ100è½®ï¼Œç¡®ä¿æ•ˆæœ
    learning_rate=3e-4,    # å­¦ä¹ ç‡
    save_steps=200,
    use_cpu=not torch.cuda.is_available() 
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

trainer.train()

print(f"âœ… çŒ«å¨˜è°ƒæ•™å®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨ {output_dir}")
trainer.save_model(output_dir)
print("ğŸ‰ è¿è¡Œ python step3_chat.py å¼€å§‹å’Œä½ çš„æ¶©æ¶©çŒ«å¨˜å¥ˆå¥ˆäº’åŠ¨å§ï¼")
