import os
import sys

# å¯¼å…¥ç»Ÿä¸€é…ç½®
from config import setup_environment, BASE_MODEL, FINE_TUNED_MODEL_DIR, DB_FILE

# è®¾ç½®ç¯å¢ƒï¼ˆé•œåƒæºã€ç¼“å­˜è·¯å¾„ç­‰ï¼‰
setup_environment()

import gradio as gr
import sqlite3
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import difflib

# ==========================================
# é˜¶æ®µä¸ƒï¼šWeb UI (Gradio + RAG + LoRA)
# ==========================================

# é…ç½®ï¼ˆä½¿ç”¨ config.py ç»Ÿä¸€ç®¡ç†ï¼‰
base_model_name = BASE_MODEL
lora_path = FINE_TUNED_MODEL_DIR
db_file = DB_FILE

print("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")

# 1. åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(base_model_name, dtype="auto")

try:
    model = PeftModel.from_pretrained(base_model, lora_path)
    print("âœ… å¾®è°ƒæƒé‡åŠ è½½æˆåŠŸ")
except:
    print("âš ï¸ ä½¿ç”¨åŸºåº§æ¨¡å‹")
    model = base_model

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
print(f"âœ… è¿è¡Œåœ¨: {device}")

# 2. æ•°æ®åº“æ£€ç´¢
def search_database(query):
    if not isinstance(query, str):
        query = str(query)
    try:
        conn = sqlite3.connect(db_file)
        cursor = conn.cursor()
        cursor.execute("SELECT question, answer FROM faq")
        all_data = cursor.fetchall()
        conn.close()
        
        questions = [row[0] for row in all_data]
        matches = difflib.get_close_matches(query, questions, n=1, cutoff=0.4)
        
        if matches:
            for q, a in all_data:
                if q == matches[0]:
                    return f"é—®é¢˜ï¼š{q}\nç­”æ¡ˆï¼š{a}"
        return None
    except Exception as e:
        return None

# 3. ç”Ÿæˆå›ç­”
def generate_response(message, history):
    # æå–æ–‡æœ¬
    if isinstance(message, list):
        message = " ".join([item.get('text', '') for item in message if isinstance(item, dict)])
    message = str(message)
    
    # æ£€ç´¢
    retrieved = search_database(message)
    
    # æ„å»ºPrompt
    if retrieved:
        system_prompt = f"ä½ æ˜¯FutureAIå…¬å¸çš„åŠ©æ‰‹ã€‚æ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ï¼š\n{retrieved}"
        rag_status = f"âœ… æ‰¾åˆ°èµ„æ–™ï¼š\n{retrieved}"
    else:
        system_prompt = "ä½ æ˜¯FutureAIå…¬å¸çš„åŠ©æ‰‹ã€‚"
        rag_status = "âŒ æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™"
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history:
        if isinstance(msg, dict):
            messages.append({"role": msg.get("role", "user"), "content": str(msg.get("content", ""))})
    messages.append({"role": "user", "content": message})
    
    # ç”Ÿæˆ
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=256, temperature=0.7, top_p=0.9
        )
    
    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    return response, rag_status

# 4. Gradioç•Œé¢
with gr.Blocks(title="FutureAI æ™ºèƒ½åŠ©æ‰‹") as demo:
    gr.Markdown("# ğŸ¤– FutureAI æ™ºèƒ½åŠ©æ‰‹\nç»“åˆ LoRAå¾®è°ƒ + RAGçŸ¥è¯†åº“")
    
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=450)
            msg = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šWifiå¯†ç æ˜¯å¤šå°‘ï¼Ÿ")
            with gr.Row():
                submit = gr.Button("å‘é€", variant="primary")
                clear = gr.Button("æ¸…é™¤")
        
        with gr.Column(scale=1):
            rag_info = gr.Textbox(label="RAGæ£€ç´¢çŠ¶æ€", lines=8, interactive=False)
    
    def user_input(message, history):
        return "", history + [{"role": "user", "content": message}]
    
    def bot_response(history):
        if not history:
            return history, ""
        user_msg = history[-1]["content"]
        response, rag_data = generate_response(user_msg, history[:-1])
        history.append({"role": "assistant", "content": response})
        return history, rag_data
    
    msg.submit(user_input, [msg, chatbot], [msg, chatbot]).then(
        bot_response, [chatbot], [chatbot, rag_info]
    )
    submit.click(user_input, [msg, chatbot], [msg, chatbot]).then(
        bot_response, [chatbot], [chatbot, rag_info]
    )
    clear.click(lambda: ([], ""), None, [chatbot, rag_info])

if __name__ == "__main__":
    import os
    # ç¦ç”¨ä»£ç†ï¼Œé¿å… localhost è®¿é—®è¢«æ‹¦æˆª
    os.environ["NO_PROXY"] = "localhost,127.0.0.1"
    os.environ["no_proxy"] = "localhost,127.0.0.1"
    
    print("å¯åŠ¨ Web æœåŠ¡...")
    demo.launch(server_name="127.0.0.1", server_port=7860, inbrowser=True)
