# AI LLM Project 使用指南

本项目是一个完整的本地大语言模型（LLM）应用开发流程示例，涵盖了从环境配置、数据生成、模型微调（Fine-tuning）、RAG（检索增强生成）到 Web 界面展示的全过程。

本项目已针对本地离线环境进行优化，解决了网络连接和 SSL 证书问题。

## 1. 环境准备

确保你已经安装了 Python 3.10+，并激活了虚拟环境。

```powershell
# 激活虚拟环境 (Windows PowerShell)
.\venv_d\Scripts\Activate.ps1
```

**快速启动**：可直接运行 `.\start.ps1` 一键激活环境并配置变量。

## 2. 项目配置 (`config.py`)

**核心配置文件**：`config.py`

所有脚本均通过此文件获取配置。目前已配置为**完全离线模式**，直接加载本地已下载的模型，无需联网。

| 配置项 | 路径 |
|--------|------|
| 基座模型 | `D:/AI_LLM_Project/models/qwen/Qwen2___5-0___5B-Instruct` |
| Embedding模型 | `D:/AI_LLM_Project/models/modelscope/BAAI/bge-small-zh-v1___5` |
| 模型缓存目录 | `D:/AI_LLM_Project/models` |
| 微调模型输出 | `./fine_tuned_model` |
| 向量数据库 | `./chroma_db` |

> **注意**：如果你移动了项目或模型文件夹，请务必修改 `config.py` 中的路径。

## 3. 详细运行步骤

请按照以下顺序运行脚本，体验完整的 LLM 开发流程。

### 第一阶段：基础验证与数据准备

**Step 1: 测试基座模型** ✅
验证本地模型是否能正常加载和运行。
```powershell
python step1_test_model.py
```
*预期结果*：成功加载 Qwen2.5-0.5B 模型，并对测试问题做出回答。

**Step 2: 生成训练数据** ✅
生成用于微调的模拟数据集（JSONL格式）。
```powershell
python step2_create_data.py
```
*预期结果*：生成 `train_data.jsonl` 文件（6条），包含关于"FutureAI"公司的问答数据。

### 第二阶段：模型微调 (Fine-tuning)

**Step 3: 启动微调** ⏱️
使用 LoRA 技术对模型进行轻量级微调，使其学习特定知识。
```powershell
python step3_finetune.py
```
*预期结果*：开始训练流程（约需几分钟），训练完成后在 `fine_tuned_model` 目录下生成适配器文件。

> **提示**：step3 会自动检测数据文件。如果已运行 step6 生成了 `train_data_large.jsonl`，则使用大数据集（47条，效果更好）；否则使用 step2 生成的小数据集（6条，用于快速体验）。

**Step 4: 测试微调模型** ✅
加载微调后的模型，验证其是否学会了新知识。
```powershell
python step4_demo.py
```
*预期结果*：模型能准确回答关于"FutureAI"公司的特定问题（如 Wifi 密码：`Welcome2025`），证明微调生效。

### 第三阶段：RAG (检索增强生成)

**Step 5: 基础 RAG 演示** ✅
演示如何结合外部数据库来增强模型回答（基于字符串相似度匹配）。
```powershell
python step5_rag_demo.py
```
*预期结果*：系统先从数据库检索相关资料，再让模型根据资料回答问题。

**Step 6: 创建大型数据库** ✅
生成更丰富的企业知识库（47条问答，覆盖多个业务场景）。
```powershell
python step6_create_large_db.py
```
*预期结果*：
- 重建 `company_data.db` 数据库
- 生成 `train_data_large.jsonl` 训练数据

**Step 8: 向量数据库 RAG** ✅
构建向量数据库，实现基于语义搜索的高级 RAG。
```powershell
python step8_vector_rag.py
```
*预期结果*：
1. 加载 BGE Embedding 模型
2. 将数据向量化并存入 ChromaDB
3. 演示语义检索并生成回答

> **提示**：如需重建向量库，删除 `chroma_db` 文件夹后重新运行。

### 第四阶段：应用部署与优化

**Step 7: Web UI 界面** ✅
启动一个基于 Gradio 的网页聊天界面，结合 LoRA 微调 + RAG 知识库。
```powershell
python step7_web_ui.py
```
*预期结果*：
- 启动本地服务器：http://127.0.0.1:7860
- 浏览器自动打开聊天界面
- 右侧显示 RAG 检索状态

**Step 9: 模型量化对比** ✅
对比 FP16 / 8bit / 4bit 量化效果，展示显存节省情况。
```powershell
python step9_quantization.py
```
*预期结果*：
| 模式 | 显存占用 | 节省比例 |
|------|---------|---------|
| FP16 | ~950 MB | 基准 |
| 8bit | ~611 MB | ↓36% |
| 4bit | ~445 MB | ↓53% |

## 4. 项目文件说明

| 文件 | 说明 |
|------|------|
| `config.py` | 统一配置文件（模型路径、数据库路径等） |
| `start.ps1` | 快速启动脚本（激活环境+设置变量） |
| `train_data.jsonl` | 小训练集（6条） |
| `train_data_large.jsonl` | 大训练集（47条） |
| `company_data.db` | SQLite 知识库数据库 |
| `fine_tuned_model/` | LoRA 微调权重输出目录 |
| `chroma_db/` | ChromaDB 向量数据库目录 |

## 5. 常见问题 (FAQ)

**Q: 运行脚本时提示 SSL 错误或连接超时？**
A: 本项目已配置为加载本地绝对路径的模型文件，理论上不会再尝试联网。如果出现此错误，请检查 `config.py` 中的 `BASE_MODEL` 和 `EMBEDDING_MODEL` 路径是否指向了真实存在的文件夹。

**Q: 显存不足 (OOM)？**
A: 本项目使用的是 0.5B 的小模型，通常占用显存约 1GB。如果遇到问题，尝试关闭其他占用显存的程序。

**Q: Step 7 Web UI 启动失败，提示 502 错误？**
A: 这通常是系统代理设置影响了 localhost 访问。最新版本已自动禁用代理，如仍有问题，可尝试：
- 关闭系统代理/VPN
- 或在环境变量中设置 `NO_PROXY=localhost,127.0.0.1`

**Q: Step 8 向量检索结果不准确？**
A: 可以删除 `chroma_db` 文件夹后重新运行，确保向量库与数据库内容同步。

**Q: 如何获得更好的微调效果？**
A: 
1. 先运行 `step6_create_large_db.py` 生成大数据集
2. 再运行 `step3_finetune.py` 使用 47 条数据训练
3. 训练轮数可在 step3 中调整（默认 60 轮）

## 6. 技术栈

- **基座模型**: Qwen2.5-0.5B-Instruct
- **Embedding**: BGE-small-zh-v1.5
- **微调技术**: LoRA (PEFT)
- **向量数据库**: ChromaDB
- **Web框架**: Gradio
- **量化工具**: BitsAndBytes

---
*最后更新时间: 2025-11-27*
