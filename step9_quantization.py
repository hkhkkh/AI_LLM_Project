import os
import sys

# å¯¼å…¥ç»Ÿä¸€é…ç½®
from config import setup_environment, BASE_MODEL

# è®¾ç½®ç¯å¢ƒï¼ˆé•œåƒæºã€ç¼“å­˜è·¯å¾„ç­‰ï¼‰
setup_environment()

import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ==========================================
# é˜¶æ®µä¹ï¼š4bit é‡åŒ–å¯¹æ¯”
# ==========================================

model_name = BASE_MODEL

print("="*50)
print("é˜¶æ®µä¹ï¼šé‡åŒ–æŠ€æœ¯å¯¹æ¯” (FP16 vs 8bit vs 4bit)")
print("="*50)

# ç³»ç»Ÿä¿¡æ¯
print(f"\nGPU: {'âœ… ' + torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'âŒ æœªæ£€æµ‹åˆ°'}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"

# æµ‹è¯•prompt
test_prompt = "FutureAIå…¬å¸çš„Wifiå¯†ç æ˜¯å¤šå°‘ï¼Ÿ"
messages = [{"role": "user", "content": test_prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def test_model(model, name):
    """æµ‹è¯•æ¨¡å‹æ˜¾å­˜å’Œé€Ÿåº¦"""
    inputs = tokenizer([text], return_tensors="pt").to(device)
    mem = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
    
    start = time.time()
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=50)
    elapsed = time.time() - start
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\n[{name}]")
    print(f"  æ˜¾å­˜: {mem:.0f} MB | è€—æ—¶: {elapsed:.2f}s")
    print(f"  å›ç­”: {response[-80:]}")
    return mem, elapsed

results = {}

# æ–¹æ¡ˆA: FP16
print("\n" + "-"*50)
print("æµ‹è¯• FP16 (å…¨ç²¾åº¦)")
model_fp = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
if device == "cuda":
    model_fp = model_fp.to(device)
results['FP16'] = test_model(model_fp, "FP16")
del model_fp
torch.cuda.empty_cache() if torch.cuda.is_available() else None

# æ–¹æ¡ˆB: 8bit
print("\n" + "-"*50)
print("æµ‹è¯• 8bit é‡åŒ–")
config_8bit = BitsAndBytesConfig(load_in_8bit=True)
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, quantization_config=config_8bit, device_map="auto"
)
results['8bit'] = test_model(model_8bit, "8bit")
del model_8bit
torch.cuda.empty_cache() if torch.cuda.is_available() else None

# æ–¹æ¡ˆC: 4bit
print("\n" + "-"*50)
print("æµ‹è¯• 4bit é‡åŒ– (NF4)")
config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, quantization_config=config_4bit, device_map="auto"
)
results['4bit'] = test_model(model_4bit, "4bit")

# æ€»ç»“
print("\n" + "="*50)
print("é‡åŒ–æ•ˆæœå¯¹æ¯”")
print("="*50)

fp_mem, fp_time = results['FP16']
for name, (mem, t) in results.items():
    mem_save = (1 - mem/fp_mem) * 100 if fp_mem > 0 else 0
    print(f"{name:5s}: æ˜¾å­˜ {mem:6.0f} MB (â†“{mem_save:4.0f}%) | é€Ÿåº¦ {t:.2f}s")

print("\nğŸ’¡ ç»“è®ºï¼š4bité‡åŒ–å¯èŠ‚çœçº¦75%æ˜¾å­˜ï¼Œé€Ÿåº¦ç•¥æ…¢ä½†å¯æ¥å—")
print("âœ… é˜¶æ®µä¹å®Œæˆï¼")
